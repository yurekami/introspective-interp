target_model_path: meta-llama/Llama-3.1-8B
model_path: meta-llama/Llama-3.1-70B
continuous_tokens:
  "begin_continuous": "<|reserved_special_token_10|>"
  "end_continuous": "<|reserved_special_token_11|>"
  "continuous_rep": "<|reserved_special_token_12|>"
output_dir: /data/artifacts/bzl/autointerp/checkpoints/SAE_feature_explainer_basellama_basellama70b_131k_dec_lora_pretrainedembed
cache_dir: /data/artifacts/bzl/autointerp/checkpoints/
embed_proj_path: /data/artifacts/bzl/autointerp/alignment_outputs/llama_3.1_70b_llama_3.1_8b_base/best_alignment_model.pt
train:
  num_samples: 100000000
  batch_size: 24
  save_strategy: steps
  save_total_limit: 10
  save_steps: 8000
  learning_rate: !!float 5e-5
  num_epochs: 10
  eval_strategy: steps
  eval_steps: 8000
  peft_lora: true
  lora_r: 128
  dataset: ["sae_explanations"]
  evaluation_type: mixed
  explanation_dir: /data/artifacts/bzl/autointerp/datasets/SAE_feature_explanations_llama3.1_8b/
  split_keys: explanations/{split}_layer_feature_idxs.pkl
  split_explanations_data: explanations/{split}_explanations.pkl
  sae_save_path: features
  layers: [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31]
  bf16: true
  tasks:
    features_explain:
      question_types:
        generative_explanation:
          evaluation_type: semantic_similarity
          weight: 1.0
          layers: [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31]
          prompts:
            - "At layer {layer}, {begin_continuous}{feature}{end_continuous} encodes "
            - "{begin_continuous}{feature}{end_continuous} activates at layer {layer} for "
            - "We can describe {begin_continuous}{feature}{end_continuous} at layer {layer} as encoding "
            - "Generate a description of this feature at layer {layer}: {begin_continuous}{feature}{end_continuous}. "
            - "What does {begin_continuous}{feature}{end_continuous} encode at layer {layer}? "
            - "{begin_continuous}{feature}{end_continuous} activates at layer {layer} for inputs with the following features: "

test:
  batch_size: 24
  tasks:
    features_explain:
      num_samples: 1600
      dataset: ["custom_explanations"]
      simulator_path: Transluce/features_explain_llama3.1_8b_simulator
      evaluation_type: simulator_correlation
      explanation_dir: /data/artifacts/bzl/autointerp/datasets/fineweb_llama_3.1_8b_95seqlen_fineweb_acts_grads_-1.0/
      split_keys: "{split}_indices.json"
      split_explanations_data: "{split}_indices.json"
      exemplar_config_path: exemplar_config.json
      vectors_save_path: "{split}_vectors.pt"
