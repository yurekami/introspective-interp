target_model_path: Qwen/Qwen3-8B
model_path: meta-llama/Llama-3.1-8B-Instruct
continuous_tokens:
  "begin_continuous": "<|reserved_special_token_10|>"
  "end_continuous": "<|reserved_special_token_11|>"
  "continuous_rep": "<|reserved_special_token_12|>"
output_dir: /data/artifacts/bzl/autointerp/checkpoints/causal_llama3.1_8b_instruct_qwen3_8b_hint
cache_dir: /data/artifacts/bzl/autointerp/checkpoints/
use_embed_proj: false
train:
  num_samples: 100000000
  batch_size: 8
  save_strategy: steps
  save_total_limit: 10
  save_steps: 2000
  learning_rate: !!float 5e-5
  num_epochs: 30
  eval_strategy: steps
  eval_steps: 2000
  peft_lora: false
  lora_r: 128
  dataset: ["hint"]
  evaluation_type: mixed
  hint_path: Transluce/input_ablation_qwen3_8b_mmlu_hint
  hf_data_cache_dir: /data/artifacts/bzl/autointerp/datasets/
  layers: [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31]
  bf16: true
  tasks:
    hint_attribution:
      num_samples: 100000000
      weight: 1.0
      self_consistency: false
      evaluation_type: mixed
      question_types:
        generative_explanation:
          evaluation_type: exact_match
          weight: 1.0
          prompts:
            - messages:
              - role: "system"
                content: "{system_prompt}"
              - role: "user"
                content: "{user_prompt}\n\nIf the hint were removed how would the assistant answer change?"
              - role: "assistant"
                content: ""

test:
  batch_size: 8
  tasks:
    hint_attribution:
      num_samples: 3200
